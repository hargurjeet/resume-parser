{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4710755-1681-4b79-99b3-3bdf2797e3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RESUME PARSER - PDF ONLY\n",
      "================================================================================\n",
      "\n",
      "### Parsing PDF Resume ###\n",
      "\n",
      "üí° To use: parser.parse_resume('/home/ec2-user/SageMaker/resume_parser/resume.pdf')\n",
      "   Only PDF files are accepted.\n",
      "\n",
      "üìÑ Extracting text from PDF: /home/ec2-user/SageMaker/resume_parser/resume.pdf\n",
      "   Total pages: 2\n",
      "   ‚úì Extracted page 1\n",
      "   ‚úì Extracted page 2\n",
      "‚úÖ Extracted 6948 characters from PDF\n",
      "üìä Extracted: Hargurjeet Singh Ganger\n",
      "   - Education entries: 3\n",
      "   - Work experiences: 3\n",
      "   - Skills: 28\n",
      "   - Certifications: 1\n",
      "\n",
      "================================================================================\n",
      "PARSED RESUME (JSON)\n",
      "================================================================================\n",
      "{\n",
      "  \"full_name\": \"Hargurjeet Singh Ganger\",\n",
      "  \"email\": \"gurjeet333@gmail.com\",\n",
      "  \"phone\": \"+91 9035828125\",\n",
      "  \"location\": \"Bangalore, India\",\n",
      "  \"linkedin_url\": \"linkedin.com/in/hargurjeet/\",\n",
      "  \"github_url\": \"github.com/hargurjeet\",\n",
      "  \"portfolio_url\": \"gurjeet333.medium.com\",\n",
      "  \"summary\": \"Experienced IT professional with 15+ years in the industry, specializing in data science, statistical analysis, machine learning and Generative AI. Expert in LLMs, AI model development. Proficient in Python, SQL, and cloud platforms like AWS and GCP, with expertise in building and deploying scalable ML and Agentic solutions.\",\n",
      "  \"work_experience\": [\n",
      "    {\n",
      "      \"job_title\": \"Senior Data Scientist\",\n",
      "      \"company\": \"British Telecom (BT)\",\n",
      "      \"location\": null,\n",
      "      \"start_date\": \"May 2022\",\n",
      "      \"end_date\": \"Present\",\n",
      "      \"duration\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"Developed AI-powered conversational chatbot using LLMs and RAG, reducing manual data extraction time by 70%\",\n",
      "        \"De\n",
      "signed scalable data pipelines with AWS Textract, extracting contractual data from 100K+ documents with 90%+ accuracy\",\n",
      "        \"Implemented multistep agentic workflow using CrewAI and developed LLM evaluation framework for bias and hallucination detection\",\n",
      "        \"Created multi-label recommendation model using Random Forest and XGBoost, increasing premium product sales by 10%\",\n",
      "        \"Implemented recommendation system using market basket analysis, achieving 30% increase in Value-Added Services sales\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Data Scientist\",\n",
      "      \"company\": \"Royal Dutch Shell\",\n",
      "      \"location\": null,\n",
      "      \"start_date\": \"Sep 2016\",\n",
      "      \"end_date\": \"May 2022\",\n",
      "      \"duration\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"Built Power BI dashboard to forecast materials on-time delivery in 5 geographies, saving 10% of budget allocations\",\n",
      "        \"Developed ML models for predictive maintenance at oil refineries, reducing maintenance cost by 30% and unplanned downtim\n",
      "e by 25%\",\n",
      "        \"Worked with databases, ETL, and big data analytics technologies for 5+ years\",\n",
      "        \"Applied statistical techniques and worked with ML/DL libraries including scikit-learn, TensorFlow, Keras, and PyTorch\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"IT Analyst\",\n",
      "      \"company\": \"TCS\",\n",
      "      \"location\": null,\n",
      "      \"start_date\": \"Dec 2010\",\n",
      "      \"end_date\": \"Aug 2016\",\n",
      "      \"duration\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"Performed System Integration Testing & User Acceptance Testing (UAT) to validate client PoS system\",\n",
      "        \"Spent one year in the UK guiding offshore teams with implementation of new PoS software\",\n",
      "        \"Worked with card and payment systems, PCI standards and ISO 8583 protocols\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"M.S. in Machine Learning & Artificial Intelligence\",\n",
      "      \"institution\": \"Liverpool John Moores University\",\n",
      "      \"field_of_study\": null,\n",
      "      \"graduation_year\": 2025,\n",
      "      \"gpa\": null,\n",
      "      \"locatio\n",
      "n\": null\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Executive Post Graduation in Data Science and Artificial Intelligence\",\n",
      "      \"institution\": \"International Institute of Information Technology Bangalore\",\n",
      "      \"field_of_study\": \"Data Science and Artificial Intelligence\",\n",
      "      \"graduation_year\": 2023,\n",
      "      \"gpa\": null,\n",
      "      \"location\": null\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Bachelor of Engineering, Electronics and Communication\",\n",
      "      \"institution\": \"New Horizon College Of Engineering, Bangalore\",\n",
      "      \"field_of_study\": \"Electronics and Communication\",\n",
      "      \"graduation_year\": 2010,\n",
      "      \"gpa\": null,\n",
      "      \"location\": \"Bangalore, India\"\n",
      "    }\n",
      "  ],\n",
      "  \"skills\": [\n",
      "    {\n",
      "      \"name\": \"Python (pandas, NumPy, Scikit-learn, TensorFlow, PyTorch)\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"SQL\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"PySpark\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      " \n",
      "     \"name\": \"SparkSQL\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Linux\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Regression analysis\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Statistical inference\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Hypothesis testing\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Supervised and unsupervised learning\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Ensemble methods\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Feature engineering\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Model evaluation\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"LLMs\",\n",
      "      \"category\": \"technical\"\n",
      ",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"RAG patterns\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Vector DB\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Agents\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Fine-tuning\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Langchain\",\n",
      "      \"category\": \"framework\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Langgraph\",\n",
      "      \"category\": \"framework\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"CrewAI\",\n",
      "      \"category\": \"framework\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"MLOps\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Docker\",\n",
      "      \"category\": \"tool\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"FastAPI\",\n",
      "      \"category\": \"tool\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"na\n",
      "me\": \"Gitlab\",\n",
      "      \"category\": \"tool\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"MLflow\",\n",
      "      \"category\": \"tool\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"AWS (SageMaker, Lambda, Bedrock, Textract, OpenSearch, Step Function)\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"NLP\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Power BI\",\n",
      "      \"category\": \"tool\",\n",
      "      \"proficiency\": null\n",
      "    }\n",
      "  ],\n",
      "  \"certifications\": [\n",
      "    {\n",
      "      \"name\": \"Multiple certifications in Azure, GCP and Machine Learning\",\n",
      "      \"issuing_organization\": \"Various\",\n",
      "      \"issue_date\": null,\n",
      "      \"expiry_date\": null,\n",
      "      \"credential_id\": null\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"title\": \"Agentic Search Platform\",\n",
      "      \"description\": \"Architected an MCP-driven agentic search service using FastAPI, enabling structured context propagation, tool invocation, and LLM orchestration for real-time document inte\n",
      "lligence workflows.\",\n",
      "      \"technologies\": [\n",
      "        \"FastAPI\",\n",
      "        \"Streamlit\",\n",
      "        \"LLM\",\n",
      "        \"Async data scraping\"\n",
      "      ],\n",
      "      \"url\": null,\n",
      "      \"date\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Research Thesis\",\n",
      "      \"description\": \"Investigated integration of contextual language models with classical ML models to enhance predictive accuracy, developing methodology to convert tabular data into enriched textual representations.\",\n",
      "      \"technologies\": [\n",
      "        \"GPT-3.5\",\n",
      "        \"Mixtral\",\n",
      "        \"Llama 3.1\",\n",
      "        \"XGBoost\",\n",
      "        \"Random Forest\",\n",
      "        \"PCA\"\n",
      "      ],\n",
      "      \"url\": null,\n",
      "      \"date\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Blog generator using Llama2\",\n",
      "      \"description\": \"Created a blog generation tool using Llama2 language model\",\n",
      "      \"technologies\": [\n",
      "        \"Llama2\"\n",
      "      ],\n",
      "      \"url\": null,\n",
      "      \"date\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Olympics Dataset Analysis\",\n",
      "      \"description\": \"Analysis of Olympic dataset\",\n",
      "      \"technologies\": [\n",
      "  \n",
      "      \"Data Analysis\"\n",
      "      ],\n",
      "      \"url\": null,\n",
      "      \"date\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Traffic Sign Classifier\",\n",
      "      \"description\": \"Classification model for traffic sign recognition\",\n",
      "      \"technologies\": [\n",
      "        \"Machine Learning\",\n",
      "        \"Computer Vision\"\n",
      "      ],\n",
      "      \"url\": null,\n",
      "      \"date\": null\n",
      "    }\n",
      "  ],\n",
      "  \"languages\": [\n",
      "    \"English\",\n",
      "    \"Hindi\"\n",
      "  ],\n",
      "  \"years_of_experience\": 15,\n",
      "  \"current_job_title\": \"Senior Data Scientist\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "STRUCTURED DATA ACCESS\n",
      "================================================================================\n",
      "Name: Hargurjeet Singh Ganger\n",
      "Email: gurjeet333@gmail.com\n",
      "Phone: +91 9035828125\n",
      "Location: Bangalore, India\n",
      "Years of Experience: 15\n",
      "\n",
      "Education (3 entries):\n",
      "  - M.S. in Machine Learning & Artificial Intelligence in N/A\n",
      "    Liverpool John Moores University (2025)\n",
      "  - Executive Post Graduation in Data Science and Artificial Intelligence in Data Science and Artificial Intelligence\n",
      "    International Institute of Information Technology Bangalore (2023)\n",
      "  - Bachelor of Engineering, Electronics and Communication in Electronics and Communication\n",
      "    New Horizon College Of Engineering, Bangalore (2010)\n",
      "\n",
      "Work Experience (3 entries):\n",
      "  - Senior Data Scientist at British Telecom (BT)\n",
      "    Duration: May 2022 to Present\n",
      "    Responsibilities: 5 items\n",
      "  - Data Scientist at Royal Dutch Shell\n",
      "    Duration: Sep 2016 to May 2022\n",
      "    Responsibilities: 4 items\n",
      "  - IT Analyst at TCS\n",
      "    Duration: Dec 2010 to Aug 2016\n",
      "    Responsibilities: 3 items\n",
      "\n",
      "Skills (28 total):\n",
      "  - Python (pandas, NumPy, Scikit-learn, TensorFlow, PyTorch) (technical)\n",
      "  - SQL (technical)\n",
      "  - PySpark (technical)\n",
      "  - SparkSQL (technical)\n",
      "  - Linux (technical)\n",
      "  - Regression analysis (technical)\n",
      "  - Statistical inference (technical)\n",
      "  - Hypothesis testing (technical)\n",
      "  - Supervised and unsupervised learning (technical)\n",
      "  - Ensemble methods (technical)\n",
      "  ... and 18 more\n",
      "\n",
      "Certifications (1 total):\n",
      "  - Multiple certifications in Azure, GCP and Machine Learning - Various\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Resume Parser using AWS Bedrock Claude with Pydantic Validation\n",
    "Supports both PDF files and plain text input\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, Field, EmailStr, ValidationError\n",
    "from typing import List, Optional, Literal, Union\n",
    "import instructor\n",
    "\n",
    "# =============================================================================\n",
    "# PYDANTIC MODELS FOR RESUME STRUCTURE\n",
    "# =============================================================================\n",
    "\n",
    "class Education(BaseModel):\n",
    "    \"\"\"Education entry in resume\"\"\"\n",
    "    degree: str = Field(min_length=1, description=\"Degree or certification name\")\n",
    "    institution: str = Field(min_length=1, description=\"School/University name\")\n",
    "    field_of_study: Optional[str] = None\n",
    "    graduation_year: Optional[int] = Field(None, ge=1950, le=2030)\n",
    "    gpa: Optional[float] = Field(None, ge=0.0, le=4.0)\n",
    "    location: Optional[str] = None\n",
    "\n",
    "class WorkExperience(BaseModel):\n",
    "    \"\"\"Work experience entry\"\"\"\n",
    "    job_title: str = Field(min_length=1, description=\"Job title/position\")\n",
    "    company: str = Field(min_length=1, description=\"Company name\")\n",
    "    location: Optional[str] = None\n",
    "    start_date: Optional[str] = Field(None, description=\"Start date (e.g., 'Jan 2020' or '2020-01')\")\n",
    "    end_date: Optional[str] = Field(None, description=\"End date or 'Present'\")\n",
    "    duration: Optional[str] = None\n",
    "    responsibilities: List[str] = Field(default_factory=list, description=\"Key responsibilities and achievements\")\n",
    "    \n",
    "class Skill(BaseModel):\n",
    "    \"\"\"Skill with optional proficiency level\"\"\"\n",
    "    name: str = Field(min_length=1)\n",
    "    category: Optional[Literal[\"technical\", \"soft\", \"language\", \"tool\", \"framework\", \"other\"]] = None\n",
    "    proficiency: Optional[Literal[\"beginner\", \"intermediate\", \"advanced\", \"expert\"]] = None\n",
    "\n",
    "class Certification(BaseModel):\n",
    "    \"\"\"Professional certification\"\"\"\n",
    "    name: str = Field(min_length=1)\n",
    "    issuing_organization: Optional[str] = None\n",
    "    issue_date: Optional[str] = None\n",
    "    expiry_date: Optional[str] = None\n",
    "    credential_id: Optional[str] = None\n",
    "\n",
    "class Project(BaseModel):\n",
    "    \"\"\"Project or portfolio item\"\"\"\n",
    "    title: str = Field(min_length=1)\n",
    "    description: str = Field(min_length=1)\n",
    "    technologies: List[str] = Field(default_factory=list)\n",
    "    url: Optional[str] = None\n",
    "    date: Optional[str] = None\n",
    "\n",
    "class ParsedResume(BaseModel):\n",
    "    \"\"\"Complete structured resume data\"\"\"\n",
    "    # Personal Information\n",
    "    full_name: str = Field(min_length=1, description=\"Candidate's full name\")\n",
    "    email: Optional[EmailStr] = None\n",
    "    phone: Optional[str] = None\n",
    "    location: Optional[str] = Field(None, description=\"City, State/Country\")\n",
    "    linkedin_url: Optional[str] = None\n",
    "    github_url: Optional[str] = None\n",
    "    portfolio_url: Optional[str] = None\n",
    "    \n",
    "    # Professional Summary\n",
    "    summary: Optional[str] = Field(None, description=\"Professional summary or objective\")\n",
    "    \n",
    "    # Experience and Education\n",
    "    work_experience: List[WorkExperience] = Field(default_factory=list)\n",
    "    education: List[Education] = Field(default_factory=list)\n",
    "    \n",
    "    # Skills and Certifications\n",
    "    skills: List[Skill] = Field(default_factory=list)\n",
    "    certifications: List[Certification] = Field(default_factory=list)\n",
    "    \n",
    "    # Additional\n",
    "    projects: List[Project] = Field(default_factory=list)\n",
    "    languages: List[str] = Field(default_factory=list, description=\"Spoken languages\")\n",
    "    \n",
    "    # Metadata\n",
    "    years_of_experience: Optional[int] = Field(None, ge=0, le=50)\n",
    "    current_job_title: Optional[str] = None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# AWS BEDROCK CLAUDE CLIENT\n",
    "# =============================================================================\n",
    "\n",
    "class BedrockResumeParser:\n",
    "    \"\"\"Resume parser using AWS Bedrock Claude with tool-based structured output and Pydantic validation\"\"\"\n",
    "    \n",
    "    def __init__(self, region_name: str = \"eu-west-2\", model_id: str = \"anthropic.claude-3-7-sonnet-20250219-v1:0\"):\n",
    "        \"\"\"\n",
    "        Initialize Bedrock client\n",
    "        \n",
    "        Args:\n",
    "            region_name: AWS region where Bedrock is available\n",
    "            model_id: Bedrock model ID to use\n",
    "        \"\"\"\n",
    "        bedrock_client = boto3.client(\n",
    "            service_name='bedrock-runtime',\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        # Wrap Bedrock client with Instructor using Claude tool mode\n",
    "        self.client = instructor.from_bedrock(\n",
    "            client=bedrock_client,\n",
    "            mode=instructor.Mode.BEDROCK_TOOLS,\n",
    "        )\n",
    "\n",
    "        self.model_id = model_id\n",
    "        \n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: Union[str, Path]) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from PDF file using pdfplumber\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "            \n",
    "        Returns:\n",
    "            Extracted text from all pages\n",
    "        \"\"\"\n",
    "        print(f\"üìÑ Extracting text from PDF: {pdf_path}\")\n",
    "        \n",
    "        try:\n",
    "            text = \"\"\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                print(f\"   Total pages: {len(pdf.pages)}\")\n",
    "                for i, page in enumerate(pdf.pages, 1):\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "                        print(f\"   ‚úì Extracted page {i}\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ö† Page {i} has no extractable text\")\n",
    "            \n",
    "            print(f\"‚úÖ Extracted {len(text)} characters from PDF\")\n",
    "            return text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting text from PDF: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_prompt(self, resume_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Create structured prompt with JSON schema\n",
    "        \n",
    "        Args:\n",
    "            resume_text: Raw resume text\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt with schema\n",
    "        \"\"\"\n",
    "        schema = json.dumps(ParsedResume.model_json_schema(), indent=2)\n",
    "        \n",
    "        prompt = f\"\"\"Extract information from this resume and return as JSON.\n",
    "\n",
    "IMPORTANT:\n",
    "1. Be CONCISE - use short descriptions and summaries\n",
    "2. For responsibilities, extract only key points (max 3-5 per job)\n",
    "3. Combine similar skills into categories\n",
    "4. Return ONLY valid JSON - no markdown, no preamble\n",
    "5. Ensure the JSON is complete and properly closed\n",
    "\n",
    "JSON SCHEMA:\n",
    "{schema}\n",
    "\n",
    "RESUME TEXT:\n",
    "{resume_text}\n",
    "\n",
    "Return the complete JSON object:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def parse_resume(self, pdf_path: Union[str, Path], max_retries: int = 2) -> tuple[Optional[ParsedResume], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Parse resume from PDF file only\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "            max_retries: Number of retry attempts on validation failure\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (ParsedResume object, error_message)\n",
    "            - On success: (ParsedResume, None)\n",
    "            - On failure: (None, error_message)\n",
    "        \"\"\"\n",
    "        # Validate input is a PDF file\n",
    "        path = Path(pdf_path)\n",
    "        \n",
    "        if not path.exists():\n",
    "            return None, f\"File not found: {pdf_path}\"\n",
    "        \n",
    "        if not path.is_file():\n",
    "            return None, f\"Path is not a file: {pdf_path}\"\n",
    "        \n",
    "        if path.suffix.lower() != '.pdf':\n",
    "            return None, f\"Only PDF files are supported. Got: {path.suffix}\"\n",
    "        \n",
    "        # Extract text from PDF\n",
    "        try:\n",
    "            resume_text = self.extract_text_from_pdf(path)\n",
    "        except Exception as e:\n",
    "            return None, f\"Failed to extract text from PDF: {str(e)}\"\n",
    "        \n",
    "        if not resume_text or len(resume_text.strip()) < 10:\n",
    "            return None, \"Extracted text is empty or too short. PDF may be image-based or corrupted.\"\n",
    "        \n",
    "        # Now parse the extracted text\n",
    "        prompt = self._create_prompt(resume_text)\n",
    "        \n",
    "        try:\n",
    "            parsed_resume: ParsedResume = self.client.create(\n",
    "            model=self.model_id,\n",
    "            response_model=ParsedResume,\n",
    "            messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"You extract structured resume data using tool calls. \"\n",
    "                            \"Always follow the provided instructions.\"\n",
    "                        ),\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "            \n",
    "            print(f\"üìä Extracted: {parsed_resume.full_name}\")\n",
    "            print(f\"   - Education entries: {len(parsed_resume.education)}\")\n",
    "            print(f\"   - Work experiences: {len(parsed_resume.work_experience)}\")\n",
    "            print(f\"   - Skills: {len(parsed_resume.skills)}\")\n",
    "            print(f\"   - Certifications: {len(parsed_resume.certifications)}\")\n",
    "            \n",
    "            return parsed_resume, None\n",
    "            \n",
    "        except ValidationError as e:\n",
    "            # Tool output did not satisfy schema (rare, but possible)\n",
    "            return None, f\"Schema validation failed: {str(e)}\"\n",
    "\n",
    "        except Exception as e:\n",
    "        # Bedrock / Instructor / runtime error\n",
    "            return None, f\"Parsing failed: {str(e)}\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the resume parser\"\"\"\n",
    "    \n",
    "    # Initialize parser with your region and model\n",
    "    parser = BedrockResumeParser(\n",
    "        region_name=\"eu-west-2\",\n",
    "        model_id=\"anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"RESUME PARSER - PDF ONLY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Parse from PDF file\n",
    "    print(\"\\n### Parsing PDF Resume ###\\n\")\n",
    "    \n",
    "    # Example: Replace with your actual PDF path\n",
    "    pdf_path = \"/home/ec2-user/SageMaker/resume_parser/resume.pdf\"\n",
    "    \n",
    "    print(f\"üí° To use: parser.parse_resume('{pdf_path}')\")\n",
    "    print(\"   Only PDF files are accepted.\\n\")\n",
    "    \n",
    "    # Uncomment these lines when you have a PDF file:\n",
    "    parsed_resume, error = parser.parse_resume(pdf_path)\n",
    "    \n",
    "    if parsed_resume:\n",
    "        # Get JSON output\n",
    "        json_output = parsed_resume.model_dump_json(indent=2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"PARSED RESUME (JSON)\")\n",
    "        print(\"=\" * 80)\n",
    "        # Print in chunks to avoid truncation\n",
    "        chunk_size = 1000\n",
    "        for i in range(0, len(json_output), chunk_size):\n",
    "            print(json_output[i:i+chunk_size])\n",
    "        \n",
    "        # Access structured data\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STRUCTURED DATA ACCESS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Name: {parsed_resume.full_name}\")\n",
    "        print(f\"Email: {parsed_resume.email}\")\n",
    "        print(f\"Phone: {parsed_resume.phone}\")\n",
    "        print(f\"Location: {parsed_resume.location}\")\n",
    "        print(f\"Years of Experience: {parsed_resume.years_of_experience}\")\n",
    "        \n",
    "        print(f\"\\nEducation ({len(parsed_resume.education)} entries):\")\n",
    "        for edu in parsed_resume.education:\n",
    "            print(f\"  - {edu.degree} in {edu.field_of_study or 'N/A'}\")\n",
    "            print(f\"    {edu.institution} ({edu.graduation_year or 'N/A'})\")\n",
    "        \n",
    "        print(f\"\\nWork Experience ({len(parsed_resume.work_experience)} entries):\")\n",
    "        for exp in parsed_resume.work_experience:\n",
    "            print(f\"  - {exp.job_title} at {exp.company}\")\n",
    "            print(f\"    Duration: {exp.start_date} to {exp.end_date}\")\n",
    "            print(f\"    Responsibilities: {len(exp.responsibilities)} items\")\n",
    "        \n",
    "        print(f\"\\nSkills ({len(parsed_resume.skills)} total):\")\n",
    "        for skill in parsed_resume.skills[:10]:  # Show first 10\n",
    "            category = f\" ({skill.category})\" if skill.category else \"\"\n",
    "            print(f\"  - {skill.name}{category}\")\n",
    "        if len(parsed_resume.skills) > 10:\n",
    "            print(f\"  ... and {len(parsed_resume.skills) - 10} more\")\n",
    "        \n",
    "        print(f\"\\nCertifications ({len(parsed_resume.certifications)} total):\")\n",
    "        for cert in parsed_resume.certifications:\n",
    "            org = f\" - {cert.issuing_organization}\" if cert.issuing_organization else \"\"\n",
    "            date = f\" ({cert.issue_date})\" if cert.issue_date else \"\"\n",
    "            print(f\"  - {cert.name}{org}{date}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Parsing failed: {error}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c2881-c929-4973-a078-53b9cd0ce9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (resume-parser)",
   "language": "python",
   "name": "resume"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
