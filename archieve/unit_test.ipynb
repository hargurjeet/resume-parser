{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a1df93-6036-4e75-b017-84e59f657527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(\"resume.pdf\") as pdf:\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text() + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5af136b-cd78-4bb3-94a3-294ba89ed1d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PARSING RESUME\n",
      "================================================================================\n",
      "üîÑ Calling AWS Bedrock Claude (attempt 1/3)...\n",
      "‚úÖ Received response from Claude\n",
      "üìù Raw output length: 4300 characters\n",
      "üîç Validating output with Pydantic...\n",
      "‚úÖ Validation successful!\n",
      "üìä Extracted: Hargurjeet Singh Ganger\n",
      "   - Education entries: 3\n",
      "   - Work experiences: 3\n",
      "   - Skills: 8\n",
      "   - Certifications: 1\n",
      "\n",
      "================================================================================\n",
      "PARSED RESUME (JSON)\n",
      "================================================================================\n",
      "{\n",
      "  \"full_name\": \"Hargurjeet Singh Ganger\",\n",
      "  \"email\": \"gurjeet333@gmail.com\",\n",
      "  \"phone\": \"+91 9035828125\",\n",
      "  \"location\": \"Bangalore, India\",\n",
      "  \"linkedin_url\": \"linkedin.com/in/hargurjeet/\",\n",
      "  \"github_url\": \"github.com/hargurjeet\",\n",
      "  \"portfolio_url\": \"gurjeet333.medium.com\",\n",
      "  \"summary\": \"Experienced IT professional with 15+ years in the industry, specializing in data science, statistical analysis, machine learning and Generative AI. Expert in LLMs, AI model development. Proficient in Python, SQL, and cloud platforms like AWS and GCP.\",\n",
      "  \"work_experience\": [\n",
      "    {\n",
      "      \"job_title\": \"Senior Data Scientist\",\n",
      "      \"company\": \"British Telecom (BT)\",\n",
      "      \"location\": null,\n",
      "      \"start_date\": \"May 2022\",\n",
      "      \"end_date\": \"Present\",\n",
      "      \"duration\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"Developed AI-powered conversational chatbot using LLMs and RAG, reducing manual data extraction time by 70%\",\n",
      "        \"Designed scalable data pipelines with AWS Textract and OpenSearch to process 1\n",
      "00K+ documents with 90%+ accuracy\",\n",
      "        \"Developed multi-label recommendation models increasing premium product sales by 10% and VAS sales by 30%\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Data Scientist\",\n",
      "      \"company\": \"Royal Dutch Shell\",\n",
      "      \"location\": null,\n",
      "      \"start_date\": \"Sep 2016\",\n",
      "      \"end_date\": \"May 2022\",\n",
      "      \"duration\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"Built Power BI dashboard to forecast materials on-time delivery saving 10% of budget allocations\",\n",
      "        \"Developed ML models for predictive maintenance resulting in 30% reduction in maintenance cost\",\n",
      "        \"Worked with databases, ETL, and applied statistical techniques using ML/DL libraries\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"IT Analyst\",\n",
      "      \"company\": \"TCS\",\n",
      "      \"location\": null,\n",
      "      \"start_date\": \"Dec 2010\",\n",
      "      \"end_date\": \"Aug 2016\",\n",
      "      \"duration\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"Performed System Integration Testing & User Acceptance Testing for client PoS syst\n",
      "em\",\n",
      "        \"Guided offshore teams with implementation of new PoS software in the UK\",\n",
      "        \"Worked with card and payment systems, PCI standards and ISO 8583 protocols\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"M.S. in Machine Learning & Artificial Intelligence\",\n",
      "      \"institution\": \"Liverpool John Moores University\",\n",
      "      \"field_of_study\": null,\n",
      "      \"graduation_year\": 2025,\n",
      "      \"gpa\": null,\n",
      "      \"location\": null\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Executive Post Graduation in Data Science and Artificial Intelligence\",\n",
      "      \"institution\": \"International Institute of Information Technology Bangalore\",\n",
      "      \"field_of_study\": null,\n",
      "      \"graduation_year\": 2023,\n",
      "      \"gpa\": null,\n",
      "      \"location\": null\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Bachelor of Engineering, Electronics and Communication\",\n",
      "      \"institution\": \"New Horizon College Of Engineering, Bangalore\",\n",
      "      \"field_of_study\": \"Electronics and Communication\",\n",
      "      \"graduation_year\": 2010,\n",
      "      \"gpa\": null,\n",
      "  \n",
      "    \"location\": null\n",
      "    }\n",
      "  ],\n",
      "  \"skills\": [\n",
      "    {\n",
      "      \"name\": \"Statistics & Machine Learning\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Model Evaluation & Interpretability\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Generative AI\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"LLM Frameworks (Langchain, Langgraph, CrewAI)\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"MLOps\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Cloud Platforms (AWS)\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"NLP\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Programming (Python, SQL, PySpark)\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    }\n",
      "  ],\n",
      "  \"certifications\": [\n",
      "    {\n",
      "      \"name\": \"Multiple certifica\n",
      "tions in Azure, GCP and Machine learning\",\n",
      "      \"issuing_organization\": null,\n",
      "      \"issue_date\": null,\n",
      "      \"expiry_date\": null,\n",
      "      \"credential_id\": null\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"title\": \"Research Thesis\",\n",
      "      \"description\": \"Investigated integration of contextual language models with classical ML models to enhance predictive accuracy\",\n",
      "      \"technologies\": [\n",
      "        \"GPT-3.5\",\n",
      "        \"Mixtral\",\n",
      "        \"Llama 3.1\",\n",
      "        \"XGBoost\",\n",
      "        \"Random Forest\",\n",
      "        \"PCA\"\n",
      "      ],\n",
      "      \"url\": null,\n",
      "      \"date\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Agentic Search Platform\",\n",
      "      \"description\": \"Architected MCP-driven agentic search service enabling structured context propagation and LLM orchestration\",\n",
      "      \"technologies\": [\n",
      "        \"FastAPI\",\n",
      "        \"Streamlit\",\n",
      "        \"MCP\",\n",
      "        \"LLM\"\n",
      "      ],\n",
      "      \"url\": null,\n",
      "      \"date\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Blog generator using Llama2\",\n",
      "      \"description\": \"Blog generation application using Llama2\",\n",
      "\n",
      "      \"technologies\": [\n",
      "        \"Llama2\"\n",
      "      ],\n",
      "      \"url\": null,\n",
      "      \"date\": null\n",
      "    }\n",
      "  ],\n",
      "  \"languages\": [],\n",
      "  \"years_of_experience\": 15,\n",
      "  \"current_job_title\": \"Senior Data Scientist\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "STRUCTURED DATA ACCESS\n",
      "================================================================================\n",
      "Name: Hargurjeet Singh Ganger\n",
      "Email: gurjeet333@gmail.com\n",
      "Phone: +91 9035828125\n",
      "Location: Bangalore, India\n",
      "Years of Experience: 15\n",
      "\n",
      "Education (3 entries):\n",
      "  - M.S. in Machine Learning & Artificial Intelligence in N/A\n",
      "    Liverpool John Moores University (2025)\n",
      "  - Executive Post Graduation in Data Science and Artificial Intelligence in N/A\n",
      "    International Institute of Information Technology Bangalore (2023)\n",
      "  - Bachelor of Engineering, Electronics and Communication in Electronics and Communication\n",
      "    New Horizon College Of Engineering, Bangalore (2010)\n",
      "\n",
      "Work Experience (3 entries):\n",
      "  - Senior Data Scientist at British Telecom (BT)\n",
      "    Duration: May 2022 to Present\n",
      "    Responsibilities: 3 items\n",
      "  - Data Scientist at Royal Dutch Shell\n",
      "    Duration: Sep 2016 to May 2022\n",
      "    Responsibilities: 3 items\n",
      "  - IT Analyst at TCS\n",
      "    Duration: Dec 2010 to Aug 2016\n",
      "    Responsibilities: 3 items\n",
      "\n",
      "Skills (8 total):\n",
      "  - Statistics & Machine Learning (technical)\n",
      "  - Model Evaluation & Interpretability (technical)\n",
      "  - Generative AI (technical)\n",
      "  - LLM Frameworks (Langchain, Langgraph, CrewAI) (technical)\n",
      "  - MLOps (technical)\n",
      "  - Cloud Platforms (AWS) (technical)\n",
      "  - NLP (technical)\n",
      "  - Programming (Python, SQL, PySpark) (technical)\n",
      "\n",
      "Certifications (1 total):\n",
      "  - Multiple certifications in Azure, GCP and Machine learning\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Resume Parser using AWS Bedrock Claude with Pydantic Validation\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "from pydantic import BaseModel, Field, EmailStr, ValidationError\n",
    "from typing import List, Optional, Literal\n",
    "from datetime import date\n",
    "\n",
    "# =============================================================================\n",
    "# PYDANTIC MODELS FOR RESUME STRUCTURE\n",
    "# =============================================================================\n",
    "\n",
    "class Education(BaseModel):\n",
    "    \"\"\"Education entry in resume\"\"\"\n",
    "    degree: str = Field(min_length=1, description=\"Degree or certification name\")\n",
    "    institution: str = Field(min_length=1, description=\"School/University name\")\n",
    "    field_of_study: Optional[str] = None\n",
    "    graduation_year: Optional[int] = Field(None, ge=1950, le=2030)\n",
    "    gpa: Optional[float] = Field(None, ge=0.0, le=4.0)\n",
    "    location: Optional[str] = None\n",
    "\n",
    "class WorkExperience(BaseModel):\n",
    "    \"\"\"Work experience entry\"\"\"\n",
    "    job_title: str = Field(min_length=1, description=\"Job title/position\")\n",
    "    company: str = Field(min_length=1, description=\"Company name\")\n",
    "    location: Optional[str] = None\n",
    "    start_date: Optional[str] = Field(None, description=\"Start date (e.g., 'Jan 2020' or '2020-01')\")\n",
    "    end_date: Optional[str] = Field(None, description=\"End date or 'Present'\")\n",
    "    duration: Optional[str] = None\n",
    "    responsibilities: List[str] = Field(default_factory=list, description=\"Key responsibilities and achievements\")\n",
    "    \n",
    "class Skill(BaseModel):\n",
    "    \"\"\"Skill with optional proficiency level\"\"\"\n",
    "    name: str = Field(min_length=1)\n",
    "    category: Optional[Literal[\"technical\", \"soft\", \"language\", \"tool\", \"framework\", \"other\"]] = None\n",
    "    proficiency: Optional[Literal[\"beginner\", \"intermediate\", \"advanced\", \"expert\"]] = None\n",
    "\n",
    "class Certification(BaseModel):\n",
    "    \"\"\"Professional certification\"\"\"\n",
    "    name: str = Field(min_length=1)\n",
    "    issuing_organization: Optional[str] = None\n",
    "    issue_date: Optional[str] = None\n",
    "    expiry_date: Optional[str] = None\n",
    "    credential_id: Optional[str] = None\n",
    "\n",
    "class Project(BaseModel):\n",
    "    \"\"\"Project or portfolio item\"\"\"\n",
    "    title: str = Field(min_length=1)\n",
    "    description: str = Field(min_length=1)\n",
    "    technologies: List[str] = Field(default_factory=list)\n",
    "    url: Optional[str] = None\n",
    "    date: Optional[str] = None\n",
    "\n",
    "class ParsedResume(BaseModel):\n",
    "    \"\"\"Complete structured resume data\"\"\"\n",
    "    # Personal Information\n",
    "    full_name: str = Field(min_length=1, description=\"Candidate's full name\")\n",
    "    email: Optional[EmailStr] = None\n",
    "    phone: Optional[str] = None\n",
    "    location: Optional[str] = Field(None, description=\"City, State/Country\")\n",
    "    linkedin_url: Optional[str] = None\n",
    "    github_url: Optional[str] = None\n",
    "    portfolio_url: Optional[str] = None\n",
    "    \n",
    "    # Professional Summary\n",
    "    summary: Optional[str] = Field(None, description=\"Professional summary or objective\")\n",
    "    \n",
    "    # Experience and Education\n",
    "    work_experience: List[WorkExperience] = Field(default_factory=list)\n",
    "    education: List[Education] = Field(default_factory=list)\n",
    "    \n",
    "    # Skills and Certifications\n",
    "    skills: List[Skill] = Field(default_factory=list)\n",
    "    certifications: List[Certification] = Field(default_factory=list)\n",
    "    \n",
    "    # Additional\n",
    "    projects: List[Project] = Field(default_factory=list)\n",
    "    languages: List[str] = Field(default_factory=list, description=\"Spoken languages\")\n",
    "    \n",
    "    # Metadata\n",
    "    years_of_experience: Optional[int] = Field(None, ge=0, le=50)\n",
    "    current_job_title: Optional[str] = None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# AWS BEDROCK CLAUDE CLIENT\n",
    "# =============================================================================\n",
    "\n",
    "class BedrockResumeParser:\n",
    "    \"\"\"Resume parser using AWS Bedrock Claude with Pydantic validation\"\"\"\n",
    "    \n",
    "    def __init__(self, region_name: str = \"eu-west-2\", model_id: str = \"anthropic.claude-3-7-sonnet-20250219-v1:0\"):\n",
    "        \"\"\"\n",
    "        Initialize Bedrock client\n",
    "        \n",
    "        Args:\n",
    "            region_name: AWS region where Bedrock is available\n",
    "            model_id: Bedrock model ID to use\n",
    "        \"\"\"\n",
    "        self.client = boto3.client(\n",
    "            service_name='bedrock-runtime',\n",
    "            region_name=region_name\n",
    "        )\n",
    "        self.model_id = model_id\n",
    "    \n",
    "    def _create_prompt(self, resume_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Create structured prompt with JSON schema\n",
    "        \n",
    "        Args:\n",
    "            resume_text: Raw resume text\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt with schema\n",
    "        \"\"\"\n",
    "        schema = json.dumps(ParsedResume.model_json_schema(), indent=2)\n",
    "        \n",
    "        prompt = f\"\"\"Extract information from this resume and return as JSON.\n",
    "\n",
    "IMPORTANT:\n",
    "1. Be CONCISE - use short descriptions and summaries\n",
    "2. For responsibilities, extract only key points (max 3-5 per job)\n",
    "3. Combine similar skills into categories\n",
    "4. Return ONLY valid JSON - no markdown, no preamble\n",
    "5. Ensure the JSON is complete and properly closed\n",
    "\n",
    "JSON SCHEMA:\n",
    "{schema}\n",
    "\n",
    "RESUME TEXT:\n",
    "{resume_text}\n",
    "\n",
    "Return the complete JSON object:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def parse_resume(self, resume_text: str, max_retries: int = 2) -> tuple[Optional[ParsedResume], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Parse resume text using Claude and validate with Pydantic\n",
    "        \n",
    "        Args:\n",
    "            resume_text: Raw resume text to parse\n",
    "            max_retries: Number of retry attempts on validation failure\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (ParsedResume object, error_message)\n",
    "            - On success: (ParsedResume, None)\n",
    "            - On failure: (None, error_message)\n",
    "        \"\"\"\n",
    "        prompt = self._create_prompt(resume_text)\n",
    "        \n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                # Call Claude via Bedrock\n",
    "                print(f\"üîÑ Calling AWS Bedrock Claude (attempt {attempt + 1}/{max_retries + 1})...\")\n",
    "                \n",
    "                response = self.client.invoke_model(\n",
    "                    modelId=self.model_id,\n",
    "                    contentType=\"application/json\",\n",
    "                    accept=\"application/json\",\n",
    "                    body=json.dumps({\n",
    "                        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                        \"max_tokens\": 8192,  # Increased for longer resumes\n",
    "                        \"temperature\": 0.0,  # Deterministic for structured output\n",
    "                        \"messages\": [\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": prompt\n",
    "                            }\n",
    "                        ]\n",
    "                    })\n",
    "                )\n",
    "                \n",
    "                # Parse response\n",
    "                response_body = json.loads(response['body'].read())\n",
    "                llm_output = response_body['content'][0]['text']\n",
    "                \n",
    "                print(f\"‚úÖ Received response from Claude\")\n",
    "                print(f\"üìù Raw output length: {len(llm_output)} characters\")\n",
    "                \n",
    "                # Clean potential markdown code blocks\n",
    "                llm_output = llm_output.strip()\n",
    "                if llm_output.startswith(\"```json\"):\n",
    "                    llm_output = llm_output[7:]\n",
    "                if llm_output.startswith(\"```\"):\n",
    "                    llm_output = llm_output[3:]\n",
    "                if llm_output.endswith(\"```\"):\n",
    "                    llm_output = llm_output[:-3]\n",
    "                llm_output = llm_output.strip()\n",
    "                \n",
    "                # Check if JSON is complete (basic validation)\n",
    "                if not llm_output.endswith('}'):\n",
    "                    print(f\"‚ö†Ô∏è  Warning: JSON appears truncated (doesn't end with '}}')\") \n",
    "                    print(f\"   Last 100 chars: ...{llm_output[-100:]}\")\n",
    "                    raise ValueError(\"JSON output appears incomplete/truncated\")\n",
    "                \n",
    "                # Try to parse as JSON first to check validity\n",
    "                try:\n",
    "                    json.loads(llm_output)\n",
    "                except json.JSONDecodeError as je:\n",
    "                    print(f\"‚ö†Ô∏è  JSON decode error: {je}\")\n",
    "                    print(f\"   Error at position {je.pos}\")\n",
    "                    if je.pos > 100:\n",
    "                        print(f\"   Context: ...{llm_output[je.pos-50:je.pos+50]}...\")\n",
    "                    raise\n",
    "                \n",
    "                # Validate with Pydantic\n",
    "                print(f\"üîç Validating output with Pydantic...\")\n",
    "                parsed_resume = ParsedResume.model_validate_json(llm_output)\n",
    "                \n",
    "                print(f\"‚úÖ Validation successful!\")\n",
    "                print(f\"üìä Extracted: {parsed_resume.full_name}\")\n",
    "                print(f\"   - Education entries: {len(parsed_resume.education)}\")\n",
    "                print(f\"   - Work experiences: {len(parsed_resume.work_experience)}\")\n",
    "                print(f\"   - Skills: {len(parsed_resume.skills)}\")\n",
    "                print(f\"   - Certifications: {len(parsed_resume.certifications)}\")\n",
    "                \n",
    "                return parsed_resume, None\n",
    "                \n",
    "            except ValidationError as e:\n",
    "                error_msg = f\"Validation failed on attempt {attempt + 1}\"\n",
    "                print(f\"‚ùå {error_msg}\")\n",
    "                for error in e.errors():\n",
    "                    print(f\"   - {error['loc']}: {error['msg']}\")\n",
    "                \n",
    "                if attempt < max_retries:\n",
    "                    print(f\"üîÑ Retrying with error feedback...\")\n",
    "                    # Add error feedback to prompt for next attempt\n",
    "                    prompt += f\"\\n\\nPREVIOUS ATTEMPT FAILED WITH ERRORS:\\n{str(e)}\\n\\nPlease fix these issues and return valid JSON:\"\n",
    "                else:\n",
    "                    return None, f\"Validation failed after {max_retries + 1} attempts: {str(e)}\"\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error calling Bedrock or parsing response: {str(e)}\"\n",
    "                print(f\"‚ùå {error_msg}\")\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"üîÑ Retrying...\")\n",
    "                else:\n",
    "                    return None, error_msg\n",
    "        \n",
    "        return None, \"Max retries exceeded\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the resume parser\"\"\"\n",
    "    sample_resume = text\n",
    "    # Sample resume text\n",
    "    # sample_resume = \"\"\"\n",
    "    # John Doe\n",
    "    # Email: john.doe@email.com | Phone: (555) 123-4567\n",
    "    # Location: San Francisco, CA | LinkedIn: linkedin.com/in/johndoe\n",
    "    \n",
    "    # PROFESSIONAL SUMMARY\n",
    "    # Senior Software Engineer with 8+ years of experience in full-stack development, \n",
    "    # cloud architecture, and team leadership. Proven track record of delivering scalable \n",
    "    # solutions and mentoring junior developers.\n",
    "    \n",
    "    # WORK EXPERIENCE\n",
    "    \n",
    "    # Senior Software Engineer | Tech Corp Inc. | San Francisco, CA\n",
    "    # January 2020 - Present\n",
    "    # - Led development of microservices architecture serving 5M+ daily users\n",
    "    # - Reduced API response time by 40% through optimization and caching strategies\n",
    "    # - Mentored team of 5 junior engineers and conducted code reviews\n",
    "    # - Implemented CI/CD pipeline reducing deployment time from 2 hours to 15 minutes\n",
    "    \n",
    "    # Software Engineer | StartUp XYZ | San Francisco, CA\n",
    "    # June 2017 - December 2019\n",
    "    # - Developed RESTful APIs using Python Flask and PostgreSQL\n",
    "    # - Built responsive frontend using React and Redux\n",
    "    # - Collaborated with product team on feature specifications\n",
    "    \n",
    "    # Junior Developer | Web Solutions Ltd. | Austin, TX\n",
    "    # July 2015 - May 2017\n",
    "    # - Maintained legacy PHP applications and migrated to modern framework\n",
    "    # - Fixed bugs and implemented new features based on client requirements\n",
    "    \n",
    "    # EDUCATION\n",
    "    \n",
    "    # Bachelor of Science in Computer Science\n",
    "    # University of California, Berkeley | 2011 - 2015\n",
    "    # GPA: 3.8/4.0\n",
    "    \n",
    "    # SKILLS\n",
    "    \n",
    "    # Programming Languages: Python, JavaScript, TypeScript, Java, SQL\n",
    "    # Frameworks: React, Node.js, Django, Flask, Spring Boot\n",
    "    # Cloud & DevOps: AWS (EC2, S3, Lambda), Docker, Kubernetes, Jenkins\n",
    "    # Databases: PostgreSQL, MongoDB, Redis\n",
    "    # Other: Git, Agile/Scrum, System Design, Mentoring\n",
    "    \n",
    "    # CERTIFICATIONS\n",
    "    # - AWS Certified Solutions Architect - Associate (2022)\n",
    "    # - Certified Scrum Master (CSM) (2021)\n",
    "    \n",
    "    # PROJECTS\n",
    "    # - Open Source Contributor to React ecosystem (2020-Present)\n",
    "    # - Built personal portfolio website using Next.js and deployed on Vercel\n",
    "    # \"\"\"\n",
    "    \n",
    "    # Initialize parser with your region and model\n",
    "    parser = BedrockResumeParser(\n",
    "        region_name=\"eu-west-2\",\n",
    "        model_id=\"anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    )\n",
    "    \n",
    "    # Parse resume\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PARSING RESUME\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    parsed_resume, error = parser.parse_resume(sample_resume)\n",
    "    \n",
    "    if parsed_resume:\n",
    "        # Get JSON output\n",
    "        json_output = parsed_resume.model_dump_json(indent=2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"PARSED RESUME (JSON)\")\n",
    "        print(\"=\" * 80)\n",
    "        # Print in chunks to avoid truncation\n",
    "        chunk_size = 1000\n",
    "        for i in range(0, len(json_output), chunk_size):\n",
    "            print(json_output[i:i+chunk_size])\n",
    "        \n",
    "        # Access structured data\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STRUCTURED DATA ACCESS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Name: {parsed_resume.full_name}\")\n",
    "        print(f\"Email: {parsed_resume.email}\")\n",
    "        print(f\"Phone: {parsed_resume.phone}\")\n",
    "        print(f\"Location: {parsed_resume.location}\")\n",
    "        print(f\"Years of Experience: {parsed_resume.years_of_experience}\")\n",
    "        \n",
    "        print(f\"\\nEducation ({len(parsed_resume.education)} entries):\")\n",
    "        for edu in parsed_resume.education:\n",
    "            print(f\"  - {edu.degree} in {edu.field_of_study or 'N/A'}\")\n",
    "            print(f\"    {edu.institution} ({edu.graduation_year or 'N/A'})\")\n",
    "        \n",
    "        print(f\"\\nWork Experience ({len(parsed_resume.work_experience)} entries):\")\n",
    "        for exp in parsed_resume.work_experience:\n",
    "            print(f\"  - {exp.job_title} at {exp.company}\")\n",
    "            print(f\"    Duration: {exp.start_date} to {exp.end_date}\")\n",
    "            print(f\"    Responsibilities: {len(exp.responsibilities)} items\")\n",
    "        \n",
    "        print(f\"\\nSkills ({len(parsed_resume.skills)} total):\")\n",
    "        for skill in parsed_resume.skills[:10]:  # Show first 10\n",
    "            category = f\" ({skill.category})\" if skill.category else \"\"\n",
    "            print(f\"  - {skill.name}{category}\")\n",
    "        if len(parsed_resume.skills) > 10:\n",
    "            print(f\"  ... and {len(parsed_resume.skills) - 10} more\")\n",
    "        \n",
    "        print(f\"\\nCertifications ({len(parsed_resume.certifications)} total):\")\n",
    "        for cert in parsed_resume.certifications:\n",
    "            org = f\" - {cert.issuing_organization}\" if cert.issuing_organization else \"\"\n",
    "            date = f\" ({cert.issue_date})\" if cert.issue_date else \"\"\n",
    "            print(f\"  - {cert.name}{org}{date}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ùå Parsing failed: {error}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4710755-1681-4b79-99b3-3bdf2797e3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RESUME PARSER - PDF ONLY\n",
      "================================================================================\n",
      "\n",
      "### Parsing PDF Resume ###\n",
      "\n",
      "üí° To use: parser.parse_resume('/home/ec2-user/SageMaker/resume_parser/resume.pdf')\n",
      "   Only PDF files are accepted.\n",
      "\n",
      "üìÑ Extracting text from PDF: /home/ec2-user/SageMaker/resume_parser/resume.pdf\n",
      "   Total pages: 2\n",
      "   ‚úì Extracted page 1\n",
      "   ‚úì Extracted page 2\n",
      "‚úÖ Extracted 6948 characters from PDF\n",
      "üîÑ Calling AWS Bedrock Claude (attempt 1/3)...\n",
      "‚úÖ Received response from Claude\n",
      "üìù Raw output length: 4570 characters\n",
      "üîç Validating output with Pydantic...\n",
      "‚úÖ Validation successful!\n",
      "üìä Extracted: Hargurjeet Singh Ganger\n",
      "   - Education entries: 3\n",
      "   - Work experiences: 3\n",
      "   - Skills: 8\n",
      "   - Certifications: 1\n",
      "\n",
      "================================================================================\n",
      "PARSED RESUME (JSON)\n",
      "================================================================================\n",
      "{\n",
      "  \"full_name\": \"Hargurjeet Singh Ganger\",\n",
      "  \"email\": \"gurjeet333@gmail.com\",\n",
      "  \"phone\": \"+91 9035828125\",\n",
      "  \"location\": \"Bangalore, India\",\n",
      "  \"linkedin_url\": \"linkedin.com/in/hargurjeet/\",\n",
      "  \"github_url\": \"github.com/hargurjeet\",\n",
      "  \"portfolio_url\": \"gurjeet333.medium.com\",\n",
      "  \"summary\": \"Experienced IT professional with 15+ years in the industry, specializing in data science, statistical analysis, machine learning and Generative AI. Expert in LLMs, AI model development. Proficient in Python, SQL, and cloud platforms like AWS and GCP, with expertise in building and deploying scalable ML and Agentic solutions.\",\n",
      "  \"work_experience\": [\n",
      "    {\n",
      "      \"job_title\": \"Senior Data Scientist\",\n",
      "      \"company\": \"British Telecom (BT)\",\n",
      "      \"location\": null,\n",
      "      \"start_date\": \"May 2022\",\n",
      "      \"end_date\": \"Present\",\n",
      "      \"duration\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"Developed AI-powered conversational chatbot using LLMs and RAG, reducing manual data extraction time by 70%\",\n",
      "        \"De\n",
      "signed scalable data pipelines with AWS Textract and OpenSearch to process 100K+ documents with 90%+ accuracy\",\n",
      "        \"Developed multi-label recommendation models increasing premium product sales by 10% and VAS sales by 30%\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Data Scientist\",\n",
      "      \"company\": \"Royal Dutch Shell\",\n",
      "      \"location\": null,\n",
      "      \"start_date\": \"Sep 2016\",\n",
      "      \"end_date\": \"May 2022\",\n",
      "      \"duration\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"Built Power BI dashboard to forecast materials on-time delivery saving 10% of budget allocations\",\n",
      "        \"Developed ML models for predictive maintenance reducing costs by 30% and unplanned downtime by 25%\",\n",
      "        \"Worked with databases, ETL, and applied statistical techniques using ML/DL libraries\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"IT Analyst\",\n",
      "      \"company\": \"TCS\",\n",
      "      \"location\": null,\n",
      "      \"start_date\": \"Dec 2010\",\n",
      "      \"end_date\": \"Aug 2016\",\n",
      "      \"duration\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"P\n",
      "erformed System Integration Testing & User Acceptance Testing for client PoS system\",\n",
      "        \"Spent one year in the UK guiding offshore teams with new PoS software implementation\",\n",
      "        \"Worked with card and payment systems, PCI standards and ISO 8583 protocols\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"M.S. in Machine Learning & Artificial Intelligence\",\n",
      "      \"institution\": \"Liverpool John Moores University\",\n",
      "      \"field_of_study\": null,\n",
      "      \"graduation_year\": 2025,\n",
      "      \"gpa\": null,\n",
      "      \"location\": null\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Executive Post Graduation in Data Science and Artificial Intelligence\",\n",
      "      \"institution\": \"International Institute of Information Technology Bangalore\",\n",
      "      \"field_of_study\": null,\n",
      "      \"graduation_year\": 2023,\n",
      "      \"gpa\": null,\n",
      "      \"location\": null\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Bachelor of Engineering, Electronics and Communication\",\n",
      "      \"institution\": \"New Horizon College Of Engineering, Bangalore\",\n",
      "      \"field_of\n",
      "_study\": \"Electronics and Communication\",\n",
      "      \"graduation_year\": 2010,\n",
      "      \"gpa\": null,\n",
      "      \"location\": null\n",
      "    }\n",
      "  ],\n",
      "  \"skills\": [\n",
      "    {\n",
      "      \"name\": \"Statistics & Machine Learning\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Model Evaluation & Interpretability\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Generative AI\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"LLM Frameworks (Langchain, Langgraph, CrewAI)\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"MLOps\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Cloud Platforms (AWS)\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"NLP\",\n",
      "      \"category\": \"technical\",\n",
      "      \"proficiency\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Programming (Python, SQL, PySpark)\",\n",
      "      \"category\": \"technical\",\n",
      "    \n",
      "  \"proficiency\": null\n",
      "    }\n",
      "  ],\n",
      "  \"certifications\": [\n",
      "    {\n",
      "      \"name\": \"Multiple certifications in Azure, GCP and Machine learning\",\n",
      "      \"issuing_organization\": null,\n",
      "      \"issue_date\": null,\n",
      "      \"expiry_date\": null,\n",
      "      \"credential_id\": null\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"title\": \"Agentic Search Platform\",\n",
      "      \"description\": \"Architected an MCP-driven agentic search service using FastAPI, enabling structured context propagation, tool invocation, and LLM orchestration for real-time document intelligence workflows.\",\n",
      "      \"technologies\": [\n",
      "        \"FastAPI\",\n",
      "        \"Streamlit\",\n",
      "        \"LLM\",\n",
      "        \"MCP\"\n",
      "      ],\n",
      "      \"url\": null,\n",
      "      \"date\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Research Thesis\",\n",
      "      \"description\": \"Investigated integration of contextual language models with classical ML models to enhance predictive accuracy, developing methodology to convert tabular data into enriched textual representations.\",\n",
      "      \"technologies\": [\n",
      "        \"GPT-3.5\",\n",
      "    \n",
      "    \"Mixtral\",\n",
      "        \"Llama 3.1\",\n",
      "        \"XGBoost\",\n",
      "        \"Random Forest\",\n",
      "        \"PCA\"\n",
      "      ],\n",
      "      \"url\": null,\n",
      "      \"date\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Blog generator using Llama2\",\n",
      "      \"description\": \"Blog generation application using Llama2 model\",\n",
      "      \"technologies\": [\n",
      "        \"Llama2\"\n",
      "      ],\n",
      "      \"url\": null,\n",
      "      \"date\": null\n",
      "    }\n",
      "  ],\n",
      "  \"languages\": [],\n",
      "  \"years_of_experience\": 15,\n",
      "  \"current_job_title\": \"Senior Data Scientist\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "STRUCTURED DATA ACCESS\n",
      "================================================================================\n",
      "Name: Hargurjeet Singh Ganger\n",
      "Email: gurjeet333@gmail.com\n",
      "Phone: +91 9035828125\n",
      "Location: Bangalore, India\n",
      "Years of Experience: 15\n",
      "\n",
      "Education (3 entries):\n",
      "  - M.S. in Machine Learning & Artificial Intelligence in N/A\n",
      "    Liverpool John Moores University (2025)\n",
      "  - Executive Post Graduation in Data Science and Artificial Intelligence in N/A\n",
      "    International Institute of Information Technology Bangalore (2023)\n",
      "  - Bachelor of Engineering, Electronics and Communication in Electronics and Communication\n",
      "    New Horizon College Of Engineering, Bangalore (2010)\n",
      "\n",
      "Work Experience (3 entries):\n",
      "  - Senior Data Scientist at British Telecom (BT)\n",
      "    Duration: May 2022 to Present\n",
      "    Responsibilities: 3 items\n",
      "  - Data Scientist at Royal Dutch Shell\n",
      "    Duration: Sep 2016 to May 2022\n",
      "    Responsibilities: 3 items\n",
      "  - IT Analyst at TCS\n",
      "    Duration: Dec 2010 to Aug 2016\n",
      "    Responsibilities: 3 items\n",
      "\n",
      "Skills (8 total):\n",
      "  - Statistics & Machine Learning (technical)\n",
      "  - Model Evaluation & Interpretability (technical)\n",
      "  - Generative AI (technical)\n",
      "  - LLM Frameworks (Langchain, Langgraph, CrewAI) (technical)\n",
      "  - MLOps (technical)\n",
      "  - Cloud Platforms (AWS) (technical)\n",
      "  - NLP (technical)\n",
      "  - Programming (Python, SQL, PySpark) (technical)\n",
      "\n",
      "Certifications (1 total):\n",
      "  - Multiple certifications in Azure, GCP and Machine learning\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Resume Parser using AWS Bedrock Claude with Pydantic Validation\n",
    "Supports both PDF files and plain text input\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, Field, EmailStr, ValidationError\n",
    "from typing import List, Optional, Literal, Union\n",
    "from datetime import date\n",
    "\n",
    "# =============================================================================\n",
    "# PYDANTIC MODELS FOR RESUME STRUCTURE\n",
    "# =============================================================================\n",
    "\n",
    "class Education(BaseModel):\n",
    "    \"\"\"Education entry in resume\"\"\"\n",
    "    degree: str = Field(min_length=1, description=\"Degree or certification name\")\n",
    "    institution: str = Field(min_length=1, description=\"School/University name\")\n",
    "    field_of_study: Optional[str] = None\n",
    "    graduation_year: Optional[int] = Field(None, ge=1950, le=2030)\n",
    "    gpa: Optional[float] = Field(None, ge=0.0, le=4.0)\n",
    "    location: Optional[str] = None\n",
    "\n",
    "class WorkExperience(BaseModel):\n",
    "    \"\"\"Work experience entry\"\"\"\n",
    "    job_title: str = Field(min_length=1, description=\"Job title/position\")\n",
    "    company: str = Field(min_length=1, description=\"Company name\")\n",
    "    location: Optional[str] = None\n",
    "    start_date: Optional[str] = Field(None, description=\"Start date (e.g., 'Jan 2020' or '2020-01')\")\n",
    "    end_date: Optional[str] = Field(None, description=\"End date or 'Present'\")\n",
    "    duration: Optional[str] = None\n",
    "    responsibilities: List[str] = Field(default_factory=list, description=\"Key responsibilities and achievements\")\n",
    "    \n",
    "class Skill(BaseModel):\n",
    "    \"\"\"Skill with optional proficiency level\"\"\"\n",
    "    name: str = Field(min_length=1)\n",
    "    category: Optional[Literal[\"technical\", \"soft\", \"language\", \"tool\", \"framework\", \"other\"]] = None\n",
    "    proficiency: Optional[Literal[\"beginner\", \"intermediate\", \"advanced\", \"expert\"]] = None\n",
    "\n",
    "class Certification(BaseModel):\n",
    "    \"\"\"Professional certification\"\"\"\n",
    "    name: str = Field(min_length=1)\n",
    "    issuing_organization: Optional[str] = None\n",
    "    issue_date: Optional[str] = None\n",
    "    expiry_date: Optional[str] = None\n",
    "    credential_id: Optional[str] = None\n",
    "\n",
    "class Project(BaseModel):\n",
    "    \"\"\"Project or portfolio item\"\"\"\n",
    "    title: str = Field(min_length=1)\n",
    "    description: str = Field(min_length=1)\n",
    "    technologies: List[str] = Field(default_factory=list)\n",
    "    url: Optional[str] = None\n",
    "    date: Optional[str] = None\n",
    "\n",
    "class ParsedResume(BaseModel):\n",
    "    \"\"\"Complete structured resume data\"\"\"\n",
    "    # Personal Information\n",
    "    full_name: str = Field(min_length=1, description=\"Candidate's full name\")\n",
    "    email: Optional[EmailStr] = None\n",
    "    phone: Optional[str] = None\n",
    "    location: Optional[str] = Field(None, description=\"City, State/Country\")\n",
    "    linkedin_url: Optional[str] = None\n",
    "    github_url: Optional[str] = None\n",
    "    portfolio_url: Optional[str] = None\n",
    "    \n",
    "    # Professional Summary\n",
    "    summary: Optional[str] = Field(None, description=\"Professional summary or objective\")\n",
    "    \n",
    "    # Experience and Education\n",
    "    work_experience: List[WorkExperience] = Field(default_factory=list)\n",
    "    education: List[Education] = Field(default_factory=list)\n",
    "    \n",
    "    # Skills and Certifications\n",
    "    skills: List[Skill] = Field(default_factory=list)\n",
    "    certifications: List[Certification] = Field(default_factory=list)\n",
    "    \n",
    "    # Additional\n",
    "    projects: List[Project] = Field(default_factory=list)\n",
    "    languages: List[str] = Field(default_factory=list, description=\"Spoken languages\")\n",
    "    \n",
    "    # Metadata\n",
    "    years_of_experience: Optional[int] = Field(None, ge=0, le=50)\n",
    "    current_job_title: Optional[str] = None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# AWS BEDROCK CLAUDE CLIENT\n",
    "# =============================================================================\n",
    "\n",
    "class BedrockResumeParser:\n",
    "    \"\"\"Resume parser using AWS Bedrock Claude with Pydantic validation\"\"\"\n",
    "    \n",
    "    def __init__(self, region_name: str = \"eu-west-2\", model_id: str = \"anthropic.claude-3-7-sonnet-20250219-v1:0\"):\n",
    "        \"\"\"\n",
    "        Initialize Bedrock client\n",
    "        \n",
    "        Args:\n",
    "            region_name: AWS region where Bedrock is available\n",
    "            model_id: Bedrock model ID to use\n",
    "        \"\"\"\n",
    "        self.client = boto3.client(\n",
    "            service_name='bedrock-runtime',\n",
    "            region_name=region_name\n",
    "        )\n",
    "        self.model_id = model_id\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: Union[str, Path]) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from PDF file using pdfplumber\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "            \n",
    "        Returns:\n",
    "            Extracted text from all pages\n",
    "        \"\"\"\n",
    "        print(f\"üìÑ Extracting text from PDF: {pdf_path}\")\n",
    "        \n",
    "        try:\n",
    "            text = \"\"\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                print(f\"   Total pages: {len(pdf.pages)}\")\n",
    "                for i, page in enumerate(pdf.pages, 1):\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "                        print(f\"   ‚úì Extracted page {i}\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ö† Page {i} has no extractable text\")\n",
    "            \n",
    "            print(f\"‚úÖ Extracted {len(text)} characters from PDF\")\n",
    "            return text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting text from PDF: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_prompt(self, resume_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Create structured prompt with JSON schema\n",
    "        \n",
    "        Args:\n",
    "            resume_text: Raw resume text\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt with schema\n",
    "        \"\"\"\n",
    "        schema = json.dumps(ParsedResume.model_json_schema(), indent=2)\n",
    "        \n",
    "        prompt = f\"\"\"Extract information from this resume and return as JSON.\n",
    "\n",
    "IMPORTANT:\n",
    "1. Be CONCISE - use short descriptions and summaries\n",
    "2. For responsibilities, extract only key points (max 3-5 per job)\n",
    "3. Combine similar skills into categories\n",
    "4. Return ONLY valid JSON - no markdown, no preamble\n",
    "5. Ensure the JSON is complete and properly closed\n",
    "\n",
    "JSON SCHEMA:\n",
    "{schema}\n",
    "\n",
    "RESUME TEXT:\n",
    "{resume_text}\n",
    "\n",
    "Return the complete JSON object:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def parse_resume(self, pdf_path: Union[str, Path], max_retries: int = 2) -> tuple[Optional[ParsedResume], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Parse resume from PDF file only\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "            max_retries: Number of retry attempts on validation failure\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (ParsedResume object, error_message)\n",
    "            - On success: (ParsedResume, None)\n",
    "            - On failure: (None, error_message)\n",
    "        \"\"\"\n",
    "        # Validate input is a PDF file\n",
    "        path = Path(pdf_path)\n",
    "        \n",
    "        if not path.exists():\n",
    "            return None, f\"File not found: {pdf_path}\"\n",
    "        \n",
    "        if not path.is_file():\n",
    "            return None, f\"Path is not a file: {pdf_path}\"\n",
    "        \n",
    "        if path.suffix.lower() != '.pdf':\n",
    "            return None, f\"Only PDF files are supported. Got: {path.suffix}\"\n",
    "        \n",
    "        # Extract text from PDF\n",
    "        try:\n",
    "            resume_text = self.extract_text_from_pdf(path)\n",
    "        except Exception as e:\n",
    "            return None, f\"Failed to extract text from PDF: {str(e)}\"\n",
    "        \n",
    "        if not resume_text or len(resume_text.strip()) < 10:\n",
    "            return None, \"Extracted text is empty or too short. PDF may be image-based or corrupted.\"\n",
    "        \n",
    "        # Now parse the extracted text\n",
    "        prompt = self._create_prompt(resume_text)\n",
    "        \n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                # Call Claude via Bedrock\n",
    "                print(f\"üîÑ Calling AWS Bedrock Claude (attempt {attempt + 1}/{max_retries + 1})...\")\n",
    "                \n",
    "                response = self.client.invoke_model(\n",
    "                    modelId=self.model_id,\n",
    "                    contentType=\"application/json\",\n",
    "                    accept=\"application/json\",\n",
    "                    body=json.dumps({\n",
    "                        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                        \"max_tokens\": 8192,  # Increased for longer resumes\n",
    "                        \"temperature\": 0.0,  # Deterministic for structured output\n",
    "                        \"messages\": [\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": prompt\n",
    "                            }\n",
    "                        ]\n",
    "                    })\n",
    "                )\n",
    "                \n",
    "                # Parse response\n",
    "                response_body = json.loads(response['body'].read())\n",
    "                llm_output = response_body['content'][0]['text']\n",
    "                \n",
    "                print(f\"‚úÖ Received response from Claude\")\n",
    "                print(f\"üìù Raw output length: {len(llm_output)} characters\")\n",
    "                \n",
    "                # Clean potential markdown code blocks\n",
    "                llm_output = llm_output.strip()\n",
    "                if llm_output.startswith(\"```json\"):\n",
    "                    llm_output = llm_output[7:]\n",
    "                if llm_output.startswith(\"```\"):\n",
    "                    llm_output = llm_output[3:]\n",
    "                if llm_output.endswith(\"```\"):\n",
    "                    llm_output = llm_output[:-3]\n",
    "                llm_output = llm_output.strip()\n",
    "                \n",
    "                # Check if JSON is complete (basic validation)\n",
    "                if not llm_output.endswith('}'):\n",
    "                    print(f\"‚ö†Ô∏è  Warning: JSON appears truncated (doesn't end with '}}')\") \n",
    "                    print(f\"   Last 100 chars: ...{llm_output[-100:]}\")\n",
    "                    raise ValueError(\"JSON output appears incomplete/truncated\")\n",
    "                \n",
    "                # Try to parse as JSON first to check validity\n",
    "                try:\n",
    "                    json.loads(llm_output)\n",
    "                except json.JSONDecodeError as je:\n",
    "                    print(f\"‚ö†Ô∏è  JSON decode error: {je}\")\n",
    "                    print(f\"   Error at position {je.pos}\")\n",
    "                    if je.pos > 100:\n",
    "                        print(f\"   Context: ...{llm_output[je.pos-50:je.pos+50]}...\")\n",
    "                    raise\n",
    "                \n",
    "                # Validate with Pydantic\n",
    "                print(f\"üîç Validating output with Pydantic...\")\n",
    "                parsed_resume = ParsedResume.model_validate_json(llm_output)\n",
    "                \n",
    "                print(f\"‚úÖ Validation successful!\")\n",
    "                print(f\"üìä Extracted: {parsed_resume.full_name}\")\n",
    "                print(f\"   - Education entries: {len(parsed_resume.education)}\")\n",
    "                print(f\"   - Work experiences: {len(parsed_resume.work_experience)}\")\n",
    "                print(f\"   - Skills: {len(parsed_resume.skills)}\")\n",
    "                print(f\"   - Certifications: {len(parsed_resume.certifications)}\")\n",
    "                \n",
    "                return parsed_resume, None\n",
    "                \n",
    "            except ValidationError as e:\n",
    "                error_msg = f\"Validation failed on attempt {attempt + 1}\"\n",
    "                print(f\"‚ùå {error_msg}\")\n",
    "                for error in e.errors():\n",
    "                    print(f\"   - {error['loc']}: {error['msg']}\")\n",
    "                \n",
    "                if attempt < max_retries:\n",
    "                    print(f\"üîÑ Retrying with error feedback...\")\n",
    "                    # Add error feedback to prompt for next attempt\n",
    "                    prompt += f\"\\n\\nPREVIOUS ATTEMPT FAILED WITH ERRORS:\\n{str(e)}\\n\\nPlease fix these issues and return valid JSON:\"\n",
    "                else:\n",
    "                    return None, f\"Validation failed after {max_retries + 1} attempts: {str(e)}\"\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error calling Bedrock or parsing response: {str(e)}\"\n",
    "                print(f\"‚ùå {error_msg}\")\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"üîÑ Retrying...\")\n",
    "                else:\n",
    "                    return None, error_msg\n",
    "        \n",
    "        return None, \"Max retries exceeded\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the resume parser\"\"\"\n",
    "    \n",
    "    # Initialize parser with your region and model\n",
    "    parser = BedrockResumeParser(\n",
    "        region_name=\"eu-west-2\",\n",
    "        model_id=\"anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"RESUME PARSER - PDF ONLY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Parse from PDF file\n",
    "    print(\"\\n### Parsing PDF Resume ###\\n\")\n",
    "    \n",
    "    # Example: Replace with your actual PDF path\n",
    "    pdf_path = \"/home/ec2-user/SageMaker/resume_parser/resume.pdf\"\n",
    "    \n",
    "    print(f\"üí° To use: parser.parse_resume('{pdf_path}')\")\n",
    "    print(\"   Only PDF files are accepted.\\n\")\n",
    "    \n",
    "    # Uncomment these lines when you have a PDF file:\n",
    "    parsed_resume, error = parser.parse_resume(pdf_path)\n",
    "    \n",
    "    if parsed_resume:\n",
    "        # Get JSON output\n",
    "        json_output = parsed_resume.model_dump_json(indent=2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"PARSED RESUME (JSON)\")\n",
    "        print(\"=\" * 80)\n",
    "        # Print in chunks to avoid truncation\n",
    "        chunk_size = 1000\n",
    "        for i in range(0, len(json_output), chunk_size):\n",
    "            print(json_output[i:i+chunk_size])\n",
    "        \n",
    "        # Access structured data\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STRUCTURED DATA ACCESS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Name: {parsed_resume.full_name}\")\n",
    "        print(f\"Email: {parsed_resume.email}\")\n",
    "        print(f\"Phone: {parsed_resume.phone}\")\n",
    "        print(f\"Location: {parsed_resume.location}\")\n",
    "        print(f\"Years of Experience: {parsed_resume.years_of_experience}\")\n",
    "        \n",
    "        print(f\"\\nEducation ({len(parsed_resume.education)} entries):\")\n",
    "        for edu in parsed_resume.education:\n",
    "            print(f\"  - {edu.degree} in {edu.field_of_study or 'N/A'}\")\n",
    "            print(f\"    {edu.institution} ({edu.graduation_year or 'N/A'})\")\n",
    "        \n",
    "        print(f\"\\nWork Experience ({len(parsed_resume.work_experience)} entries):\")\n",
    "        for exp in parsed_resume.work_experience:\n",
    "            print(f\"  - {exp.job_title} at {exp.company}\")\n",
    "            print(f\"    Duration: {exp.start_date} to {exp.end_date}\")\n",
    "            print(f\"    Responsibilities: {len(exp.responsibilities)} items\")\n",
    "        \n",
    "        print(f\"\\nSkills ({len(parsed_resume.skills)} total):\")\n",
    "        for skill in parsed_resume.skills[:10]:  # Show first 10\n",
    "            category = f\" ({skill.category})\" if skill.category else \"\"\n",
    "            print(f\"  - {skill.name}{category}\")\n",
    "        if len(parsed_resume.skills) > 10:\n",
    "            print(f\"  ... and {len(parsed_resume.skills) - 10} more\")\n",
    "        \n",
    "        print(f\"\\nCertifications ({len(parsed_resume.certifications)} total):\")\n",
    "        for cert in parsed_resume.certifications:\n",
    "            org = f\" - {cert.issuing_organization}\" if cert.issuing_organization else \"\"\n",
    "            date = f\" ({cert.issue_date})\" if cert.issue_date else \"\"\n",
    "            print(f\"  - {cert.name}{org}{date}\")\n",
    "     else:\n",
    "        print(f\"\\n‚ùå Parsing failed: {error}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ba0e9-6a34-4556-88c7-0bbfc85ca50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (resume-parser)",
   "language": "python",
   "name": "resume"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
